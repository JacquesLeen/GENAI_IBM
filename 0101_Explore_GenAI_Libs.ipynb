{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7cbab18d",
   "metadata": {},
   "source": [
    "## What is GenAI\n",
    "The capability of machines to derive inspiration from examples and provide content of their own making is labelled as GenAI.\n",
    "\n",
    "### Real World Impact\n",
    "GenAI has real world impact in many areas\n",
    "\n",
    "* **Art**: genAI algorithms can create stunning artworks by learning from existing paintings\n",
    "* **Music**: genAI can compose music content\n",
    "* **Content Generation**: Tools like GPTs have shown that they can provide meaningful and context aware content\n",
    "* **Virtual Assistant**: enhancement of human experience is a topic where GenAI is really proficient\n",
    "* **Code Writing**\n",
    "* **Image Synthesis**: generation of images from a text description\n",
    "* **Deepfake Detection**\n",
    "* **Entertainment**: visual avatars for games\n",
    "* **Marketing** : AI Influencers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cfd6d6c",
   "metadata": {},
   "source": [
    "## Transformers\n",
    "They are the most important architecture for LLMs, they replace the sequential processing of data with parallel processing and introduce the concept of self-attention\n",
    "\n",
    "Step in the building process and training of Transformers are\n",
    "* Tokenization: the prompt gets broken down into tokens\n",
    "* Embedding: eacht token is represented as a vector\n",
    "* Self-attention: The model computes scores to gather information concerning the relationships between tokens in the sentece\n",
    "\n",
    "This part builds the encoder. This gets processed by the model and a response is returned. The process for returning the response are\n",
    "\n",
    "* Feed Forward NN \n",
    "* Output sequence\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7efd98d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "80edfcc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "model_name = \"facebook/blenderbot-400M-distill\"\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8e9cf1ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot: Have you ever heard of xit? It's a company that creates and sells medical devices.\n"
     ]
    }
   ],
   "source": [
    "def chat_with_bot():\n",
    "    \"\"\"A simple chat function to interact with the BlenderBot model.\"\"\"\n",
    "    while True:\n",
    "        # User input\n",
    "        user_input = input(\"You: \")\n",
    "        \n",
    "        # exit conditions\n",
    "        if user_input.lower() in [\"quit\", \"exit\"]:\n",
    "            break\n",
    "\n",
    "        #tokenize input and generate response\n",
    "        inputs = tokenizer.encode(user_input, return_tensors=\"pt\")\n",
    "        outputs = model.generate(inputs, max_length=150)\n",
    "        response = tokenizer.decode(outputs[0], skip_special_tokens=True).strip()\n",
    "\n",
    "        #return response\n",
    "        print(\"Bot:\", response)\n",
    "\n",
    "chat_with_bot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a4e103fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentencepiece\n",
    "\n",
    "model_name=\"google/flan-t5-base\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "089b3fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_with_another_bot():\n",
    "    \"\"\"A simple chat function to interact with the FLAN-T5 model.\"\"\"\n",
    "    while True:\n",
    "        # User input\n",
    "        user_input = input(\"You: \")\n",
    "\n",
    "        # exit conditions\n",
    "        if user_input.lower() in [\"quit\", \"exit\"]:\n",
    "            break\n",
    "\n",
    "        #tokenize input and generate response\n",
    "        inputs = tokenizer.encode(user_input, return_tensors=\"pt\")\n",
    "        outputs = model.generate(inputs, max_length=150)\n",
    "        response = tokenizer.decode(outputs[0], skip_special_tokens=True).strip()\n",
    "\n",
    "        #return response\n",
    "        print(\"Bot:\", response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5a5ade4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot: I'm not sure.\n",
      "Bot: no\n",
      "Bot: then...\n",
      "Bot: dont just mirror what I say\n",
      "Bot: i'm gonna be a shithole\n",
      "Bot: a sexy girl\n",
      "Bot: that's disturbing\n",
      "Bot: I'm sorry, but I can't print the conversation.\n",
      "Bot: a symphony\n"
     ]
    }
   ],
   "source": [
    "chat_with_another_bot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a07e58d",
   "metadata": {},
   "source": [
    "## Excercise: write a function that works for a generic chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "75954031",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat(model_name):\n",
    "    \"\"\"A generic chat function to interact with any Seq2SeqLM model.\"\"\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "\n",
    "    while True:\n",
    "        # User input\n",
    "        user_input = input(\"You: \")\n",
    "\n",
    "        # exit conditions\n",
    "        if user_input.lower() in [\"quit\", \"exit\"]:\n",
    "            break\n",
    "\n",
    "        #tokenize input and generate response\n",
    "        inputs = tokenizer.encode(user_input, return_tensors=\"pt\")\n",
    "        outputs = model.generate(inputs, max_length=150)\n",
    "        response = tokenizer.decode(outputs[0], skip_special_tokens=True).strip()\n",
    "\n",
    "        #return response\n",
    "        print(\"Bot:\", response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ed07cd58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot: Heya\n",
      "Bot: a sexy person\n",
      "Bot: you are a sailor\n",
      "Bot: a fbi\n",
      "Bot: a fbi\n",
      "Bot: explain why\n",
      "Bot: because I wnat to know\n"
     ]
    }
   ],
   "source": [
    "# test chat with different models\n",
    "chat(\"google/flan-t5-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36198c99",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (venv)",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
